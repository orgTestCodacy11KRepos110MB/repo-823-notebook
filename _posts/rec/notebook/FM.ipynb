{"cells":[{"cell_type":"markdown","metadata":{},"source":["## FM, FFM, DFM, NFM, Wide & Deep 实现"]},{"cell_type":"markdown","metadata":{},"source":"这个 notebook 实现了下列论文中提出的模型：\n\n- Factorization Machines\n- Field-aware Factorization Machines for CTR Prediction\n- DeepFM: A Factorization-Machine based Neural Network for CTR Prediction\n- Wide & Deep Learning for Recommender Systems\n- Neural Factorization Machines for Sparse Predictive Analytics\n\n其中的代码大量参考了[rixwew/pytorch-fm](https://github.com/rixwew/pytorch-fm)， 感谢原作者。"},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":"import pandas as pd\nimport numpy as np\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader"},{"cell_type":"markdown","metadata":{},"source":["## 构造 DataSet\n","\n","这里使用 MovieLens 数据集，输入是 `(user_id, item_id)` 输出是对应评分，因为 `user_id` 和 `item_id` 都是从 1 开始编号的，所以将 `item_id` 整体做偏移。"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":"class MovieLensDataset(Dataset):\n    def __init__(self, path):\n        data = pd.read_csv(path, sep='\\t').values\n        self.items = data[:, :2].astype(np.int)\n        self.ratings = data[:, 2].astype(np.float32)\n        self.field_dims = np.max(self.items, axis=0)\n        self.items[:,1] + self.field_dims[0]\n        \n    def __len__(self):\n        return self.items.shape[0]\n    \n    def __getitem__(self, index):\n        return self.items[index], self.ratings[index]"},{"cell_type":"markdown","metadata":{},"source":["## 模型定义\n","\n","先定义一些后面会用到的基础组件："]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":"class FeaturesLinear(nn.Module):\n    def __init__(self, input_dim):\n        super().__init__()\n        self.embedding = nn.Embedding(input_dim, 1)\n        self.bias = nn.Parameter(torch.zeros((1,)))\n        \n    def forward(self, x):\n        x = self.embedding(x)\n        return torch.sum(x, dim=1) + self.bias\n\nclass FeaturesEmbedding(nn.Module):\n    def __init__(self, input_dim, embed_dim):\n        super().__init__()\n        self.embedding = nn.Embedding(input_dim+1, embed_dim)\n\n    def forward(self, x):\n        x = self.embedding(x)\n        return x\n    \nclass FeaturesCross(nn.Module):\n    def __init__(self, reduce_sum=True):\n        super().__init__()\n        self.reduce_sum = reduce_sum\n        \n    def forward(self, x):\n        square_of_sum = torch.sum(x, dim=1) ** 2\n        sum_of_square = torch.sum(x ** 2, dim=1)\n        \n        ix = square_of_sum - sum_of_square\n        \n        if self.reduce_sum:\n            ix = torch.sum(ix, dim=1, keepdim=True)\n        \n        return 0.5 * ix\n    \nclass MultiLayerPerceptron(torch.nn.Module):\n    def __init__(self, input_dim, embed_dims, dropout, output_layer=True):\n        super().__init__()\n        layers = []\n        for embed_dim in embed_dims:\n            layers.append(torch.nn.Linear(input_dim, embed_dim))\n            layers.append(torch.nn.BatchNorm1d(embed_dim))\n            layers.append(torch.nn.ReLU())\n            layers.append(torch.nn.Dropout(p=dropout))\n            input_dim = embed_dim\n        if output_layer:\n            layers.append(torch.nn.Linear(input_dim, 1))\n        self.mlp = torch.nn.Sequential(*layers)\n\n    def forward(self, x):\n        return self.mlp(x)"},{"cell_type":"markdown","metadata":{},"source":["### Linear Regression"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":"class LinearRegression(nn.Module):\n    def __init__(self, input_dim):\n        super().__init__()\n        self.embedding = nn.Embedding(input_dim, 1)\n        self.bias = nn.Parameter(torch.zeros((1,)))\n        \n    def forward(self, x):\n        w = self.embedding(x)\n        y = self.bias + torch.sum(w)\n        return y.squeeze(1)"},{"cell_type":"markdown","metadata":{},"source":["### Factorization Machine"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":"class FactorizationMachine(nn.Module):\n    def __init__(self, field_dims, embed_dim):\n        super().__init__()\n        input_dim = np.sum(field_dims)\n        self.linear = FeaturesLinear(input_dim)\n        self.embedding = FeaturesEmbedding(input_dim, embed_dim)\n        self.cross = FeaturesCross()\n    \n    def forward(self, x):\n        embed = self.embedding(x)\n        x = self.linear(x) + self.cross(embed)\n        return x.squeeze(1)"},{"cell_type":"markdown","metadata":{},"source":["### Field Aware Factorization Machine"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":"class FieldAwareFactorizationMachine(torch.nn.Module):\n    def __init__(self, field_dims, embed_dim):\n        super().__init__()\n        input_dim = sum(field_dims)\n        self.num_fields = len(field_dims)\n        self.linear = FeaturesLinear(input_dim)\n        self.embeddings = torch.nn.ModuleList([\n            torch.nn.Embedding(input_dim + 1, embed_dim) for _ in range(self.num_fields)\n        ])\n\n    def forward(self, x):\n        xs = [self.embeddings[i](x) for i in range(self.num_fields)]\n        ix = []\n        for i in range(self.num_fields - 1):\n            for j in range(i+1, self.num_fields):\n                ix.append(xs[j][:,i] * xs[i][:,j])\n        ix = torch.stack(ix, dim=1)\n        ffm = torch.sum(ix, (1, 2))\n        x = self.linear(x).squeeze(1) + ffm\n        return x"},{"cell_type":"markdown","metadata":{},"source":["### Neural Factorization Machine"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[],"source":"class NeuralFactorizationMachine(nn.Module):\n    def __init__(self, field_dims, embed_dim, mlp_dims, drop_rate=0.2):\n        super().__init__()\n        input_dim = np.sum(field_dims)\n        self.drop_rate = drop_rate\n        self.linear = FeaturesLinear(input_dim)\n        self.embedding = FeaturesEmbedding(input_dim, embed_dim)\n        self.mlp = MultiLayerPerceptron(embed_dim, mlp_dims, drop_rate)\n        \n        self.cross = torch.nn.Sequential(\n            FeaturesCross(reduce_sum=False),\n            nn.BatchNorm1d(embed_dim),\n            nn.Dropout(drop_rate)\n        )\n    \n    def forward(self, x):\n        embedding = self.embedding(x)\n        cross = self.cross(embedding)\n        x = self.linear(x) + self.mlp(cross)\n        return x.squeeze(1)"},{"cell_type":"markdown","metadata":{},"source":["### Deep Factorization Machine"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":"class DeepFactorizationMachine(torch.nn.Module):\n    def __init__(self, field_dims, embed_dim, mlp_dims, drop_rate=0.2):\n        super().__init__()\n        input_dim = np.sum(field_dims)\n        self.linear = FeaturesLinear(input_dim)\n        self.embedding = FeaturesEmbedding(input_dim, embed_dim)\n        self.cross = FeaturesCross()\n        \n        self.mlp_input_dim = len(field_dims) * embed_dim\n        self.mlp = MultiLayerPerceptron(self.mlp_input_dim, mlp_dims, drop_rate)\n\n    def forward(self, x):\n        embed = self.embedding(x)\n        \n        x = self.cross(embed) + self.mlp(embed.view(-1, self.mlp_input_dim))\n        return x.squeeze(1)"},{"cell_type":"markdown","metadata":{},"source":["### Wide & Deep"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":"class WideAndDeep(torch.nn.Module):\n    def __init__(self, field_dims, embed_dim, mlp_dims, drop_rate=0.2):\n        super().__init__()\n        input_dim = np.sum(field_dims)\n        self.linear = FeaturesLinear(input_dim)\n        self.embedding = FeaturesEmbedding(input_dim, embed_dim)\n        \n        self.mlp_input_dim = len(field_dims) * embed_dim\n        self.mlp = MultiLayerPerceptron(self.mlp_input_dim, mlp_dims, drop_rate)\n\n    def forward(self, x):\n        embed = self.embedding(x)\n        x = self.linear(x) + self.mlp(embed.view(-1, self.mlp_input_dim))\n        return x.squeeze(1)"},{"cell_type":"markdown","metadata":{},"source":["## 训练"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":"from sklearn.metrics import mean_squared_error\n\ndef train(model, optimizer, data_loader, criterion, device, log_interval=1000):\n    model.train()\n    total_loss = 0\n    for i, (x, y) in enumerate(data_loader):\n        x, y = x.to(device), y.to(device)\n        y_hat = model(x)\n        loss = criterion(y, y_hat)\n        model.zero_grad()\n        loss.backward()\n        optimizer.step()\n        total_loss += np.sqrt(loss.item())\n        if (i + 1) % log_interval == 0:\n            print('- loss:', total_loss / log_interval)\n            total_loss = 0\n            \ndef test(model, data_loader, device):\n    model.eval()\n    targets, predicts = [], []\n    with torch.no_grad():\n        for i, (x, y) in enumerate(data_loader):\n            x, y = x.to(device), y.to(device)\n            y_hat = model(x)\n            targets.extend(y.tolist())\n            predicts.extend(y_hat.tolist())\n    mse = mean_squared_error(targets, predicts)\n    return np.sqrt(mse)"},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["- loss: 2.1442796721315647\n","- loss: 1.6825360619697156\n","test rmse: 1.4161099228544758\n","- loss: 1.4198498158365154\n","- loss: 1.3025001574838495\n","test rmse: 1.1402408731351026\n","- loss: 1.1773485173122928\n","- loss: 1.1275556205756883\n","test rmse: 1.0608862612025725\n","- loss: 1.0828320220567376\n","- loss: 1.062964973177104\n","test rmse: 1.0275087781282006\n","- loss: 1.0434619229686866\n","- loss: 1.0375892681606158\n","test rmse: 1.011924632138925\n"]}],"source":"device = torch.device('cuda')\ndataset = MovieLensDataset(\"./data/ml-100k/u.data\")\n\ntrain_length = int(len(dataset) * 0.8)\nvalid_length = int(len(dataset) * 0.1)\ntest_length = len(dataset) - train_length - valid_length\n\ntrain_dataset, valid_dataset, test_dataset = torch.utils.data.random_split(\n    dataset, (train_length, valid_length, test_length))\n\ntrain_data_loader = DataLoader(train_dataset, batch_size=32)\nvalid_data_loader = DataLoader(valid_dataset, batch_size=32)\ntest_data_loader = DataLoader(test_dataset, batch_size=32)\n\n\n# model = LinearRegression(np.sum(dataset.field_dims))\n# model = FactorizationMachine(dataset.field_dims, 16)\n# model = FieldAwareFactorizationMachine(dataset.field_dims, 8)\n# model = DeepFactorizationMachine(dataset.field_dims, 16, [16, 16])\n# model = NeuralFactorizationMachine(dataset.field_dims, 64, [64])\nmodel = WideAndDeep(dataset.field_dims, 16, [16,16])\nmodel.cuda()\n\ncriterion = torch.nn.MSELoss()\noptimizer = torch.optim.Adam(params=model.parameters(), lr=0.001)\n\nfor epoch_i in range(5):\n    train(model, optimizer, train_data_loader, criterion, device)\n    rmse = test(model, test_data_loader, device)\n    print('test rmse:', rmse)"},{"cell_type":"markdown","metadata":{},"source":"下面是各个模型跑出来的效果，还没有仔细调参：\n\n|模型|RMSE|\n|:---|:----------------|\n|LR  |2.046919840803719|\n|FM  |1.643816350960632|\n|FFM |1.473343716268819|\n|DFM |1.461163142731684|\n|NFM |1.068538892587793|\n|W&D |1.011924632138925|"}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"name":"python","codemirror_mode":{"name":"ipython","version":3}},"orig_nbformat":2,"file_extension":".py","mimetype":"text/x-python","name":"python","npconvert_exporter":"python","pygments_lexer":"ipython3","version":3}}