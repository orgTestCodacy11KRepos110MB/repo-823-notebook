---
layout: post
title: 论文阅读 - Wide & Deep Learning for Recommender Systems
category: 推荐系统
tag: 推荐系统
---

本文为阅读论文 Wide & Deep Learning for Recommender Systems 时记下的笔记。

- *
{:toc}


## 背景

推荐系统给出的结果需要兼顾相关性和新颖性。推荐的内容和用户特征很匹配，就会推荐大量相关内容，时间一久，用户感到无新鲜感。推荐内容过于泛化，用户的兴趣无法满足。本文提出的算法用于 Google Play 的 APP 推荐系统。

## 线性模型

线性模型中各类特征常采用 one-hot 向量表示，比如“国家”属性，可取的值有 200 多个，要表示“国家”就采用一个 200 多个维度的向量，每一个国家占一个维度。类似地，其他属性也这样表示。如此以来，对某个事物的向量表示，就是拼接各个属性对应的 one-hot 向量，整个向量表示是非常稀疏的。

```
国家：[0 0 0 1 0 0 ...]
性别: [1 0]
```

用户安装过的 APP，可以有多个，采用 bag-of-word 表示，每个维度表示一个 APP 是否安装。

```
APP: [0 0 1 0 1 0 0 ...]
```

不同的特征之间可以进行组合，比如将国家和性别属性组合，可以表示如“中国男性”这样的组合属性。特征的组合能够给线性模型增加非线性的特征。但是也会极大地增大特征维度。

线性模型的使用的特征向量具有维度高、稀疏的特点。线性模型的形式如下：

$$
y = \mathbf{w}^T\mathbf{x} + b
$$

由于特征维度很大，往往没有足够的数据来训练模型中的每个参数。因为有的特征组合在数据集中根本就没有出现，或是数量很少。因此对数据集中未出现的情况，模型无法进行泛化。

论文中提到了 generalization 和 memorization 这两个词，我读的论文少，对这两个概念理解的还不够透彻。

说线性模型有较好的 memorization，我想是指线性模型能够较好地学习到各个特征（包含组合特征）的权重，以及学习到特征之间的相关性。

## 深度模型

深度模型，常常将某一个属性表示为一个低维的稠密向量，比如“国家”这个属性，可能会将不同的国家表示为一个长度为 10 的向量，这也常被称为 Embedding。如此以来，对某事物的向量表示就是一个较低维度且稠密的向量。然后使用深度网络模型，可以对 Embedding 中各维进行组合。

深度模型可以实现很好泛化，当训练数据较稀疏的时候，甚至不能反映训练集的特征。即，太过泛化。好处就是能够应对数据稀疏的场景，缺点常常会得出结果不够相关。

泛化大致是指，基于属性相关性的传递，发现过去没有或很少发生的新的特征组合，有利于增加推荐的多样性。

## Wide & Deep

Google 在 2016 年发布的 Wide & Deep 模型，组合了线性模型和深度模型。该模型结合了线性模型的记忆能力和深度模型的泛化能力。因为线性模型的输入是维度很高的向量，模型的输入很 Wide，Wide 出自于此。

![](https://wangyu-name.oss-cn-hangzhou.aliyuncs.com/superbed/2019/09/04/5d6f76ff451253d17822413b.jpg)


## 推荐系统框架

工业推荐系统基本都是 Matching 和 Ranking 两部分，Matching 用于从数据库中先粗略地检索出相关内容，极大地减小 item 的数量。Ranking 则对检索出的 item 做更加细致的排序，最终生成推荐。

![](https://wangyu-name.oss-cn-hangzhou.aliyuncs.com/superbed/2019/09/04/5d6f77af451253d178227f50.jpg)

这里 Google Play 采用的策略也是如此。


## Wide & Deep 模型架构

Wide 模型和 Deep 模型是联合起来训练的，如下图所示：

![](https://wangyu-name.oss-cn-hangzhou.aliyuncs.com/superbed/2019/09/04/5d6f79c0451253d178234980.jpg)

图中左边是深度模型，将连续属性做归一化。离散属性做 Embedding 后，拼接起来，输入全连接网络。

$$
a^{(l+1)}=f\left(W^{(l)} a^{(l)}+b^{(l)}\right)
$$

右边是线性模型，使用了用户安装的 APP 和曝光的 APP 以及两者的组合作为特征。

将 Wide 和 Deep 的输出进行求和然后交给 sigmoid 函数求出概率。最终使用 Logistics 损失函数（就是 Logistics Regression 用的损失函数）来作为优化目标，使用 SGD 进行训练。 

$$
P(Y=1 | \mathbf{x})=\sigma\left(\mathbf{w}_{\text {wide}}^{T}[\mathbf{x}, \phi(\mathbf{x})]+\mathbf{w}_{\text {deep}}^{T} a^{\left(l_{f}\right)}+b\right)
$$

Wide & Deep 模型和模型的集成不是一回事。集成是训练多个模型然后将结果进行集成，每个模型都使用了全部的样本特征。集成学习训练的多个模型是独立存在的，并不知道其他模型的存在。而这里用到的 Wide & Deep 联合学习，能够有效地组合各类特征，两个模型协同地优化目标函数。

## 总结

Wide & Deep 模型，在今天看来好像并不新颖，它提出了一种组合深层特征和浅层特征的方法。